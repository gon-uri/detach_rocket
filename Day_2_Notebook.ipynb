{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awBLy-xuFdgX"
      },
      "outputs": [],
      "source": [
        "!pip install scipy==1.14.0\n",
        "!pip install sktime --quiet\n",
        "!pip install pyts --quiet\n",
        "!pip install tsfresh --quiet\n",
        "!pip install git+https://github.com/gon-uri/detach_rocket --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJsfBi2tFdga"
      },
      "source": [
        "## 1) Download and Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-A3s151Fdgb"
      },
      "source": [
        "### Download a dataset from the UCR archive\n",
        "\n",
        "You can get a description of the datasets in the following webpage: https://www.timeseriesclassification.com/dataset.php"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0TlygVr0Fdgb"
      },
      "outputs": [],
      "source": [
        "# We will use the UCR dataset\n",
        "\n",
        "from detach_rocket.utils_datasets import fetch_ucr_dataset\n",
        "from pyts.datasets import ucr_dataset_list\n",
        "\n",
        "# Download Dataset\n",
        "selected_dataset = ['WormsTwoClass']\n",
        "print(\"Selected dataset:\", selected_dataset)\n",
        "current_dataset = fetch_ucr_dataset(selected_dataset[0])\n",
        "\n",
        "print(\" \")\n",
        "# You can later try other UCR datasets (First use WormsTwoClass dataset)\n",
        "dataset_list = ucr_dataset_list()\n",
        "print(\"All UCR Datasets:\", dataset_list)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert all labels to 0,1,2,3,4,etc..\n",
        "\n",
        "def convert_labels_to_integers(labels):\n",
        "    # Create a mapping of unique labels to integers\n",
        "    unique_labels = sorted(set(labels))\n",
        "    label_to_int = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "\n",
        "    # Convert labels using the mapping\n",
        "    converted_labels = np.asarray([label_to_int[label] for label in labels])\n",
        "    return converted_labels, label_to_int"
      ],
      "metadata": {
        "id": "3lUa6-o8-rdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZmK9UoCFdgc"
      },
      "source": [
        "The ECG200 dataset was formatted by R. Olszewski as part of his thesis \"Generalized feature extraction for structural pattern recognition in time-series data\" at Carnegie Mellon University, 2001. Each series traces the electrical activity recorded during one heartbeat. The two classes are a normal heartbeat and a Myocardial Infarction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwgiXngDFdgc"
      },
      "source": [
        "### Create the corresponding Train and Test matrices\n",
        "\n",
        "We unpack the dictionary downloaded form the UCR into the train and test matrices that we will use for training our models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0dVMuZfFdgc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create data matrices and remove possible rows with nans or infs\n",
        "print(f\"Dataset Matrix Shape: ( # of instances , # of channels , time series length )\")\n",
        "print(f\" \")\n",
        "\n",
        "# Train Matrix\n",
        "X_train = current_dataset['data_train'].copy()\n",
        "\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_train = ~np.isnan(X_train).any(axis=1)\n",
        "non_inf_mask_train = ~np.isinf(X_train).any(axis=1)\n",
        "mask_train = np.logical_and(non_nan_mask_train,non_inf_mask_train)\n",
        "X_train = X_train[mask_train]\n",
        "\n",
        "# Shuffle dataset to avoid any possible bias (keep indexes to shuffle the labels)\n",
        "np.random.seed(42)\n",
        "indexes = np.arange(X_train.shape[0])\n",
        "np.random.shuffle(indexes)\n",
        "X_train = X_train[indexes]\n",
        "\n",
        "# We now reshape the data into shape (num instances, num channels, num timesteps)\n",
        "# This is to input the data into the feature extractor and the deep learning model.\n",
        "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
        "print(f\"Train Matrix Shape: {X_train.shape}\")\n",
        "\n",
        "# We create the labels\n",
        "y_train = current_dataset['target_train'].copy()\n",
        "\n",
        "# Conver the labels to integers starting from zero\n",
        "y_train, label_to_int = convert_labels_to_integers(y_train)\n",
        "\n",
        "# Remove nans also form labels\n",
        "y_train = y_train[mask_train]\n",
        "\n",
        "# Shuffle the labels in the same way as the data\n",
        "y_train = y_train[indexes]\n",
        "\n",
        "print(f\" \")\n",
        "\n",
        "# Test Matrix\n",
        "X_test = current_dataset['data_test'].copy()\n",
        "\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_test = ~np.isnan(X_test).any(axis=1)\n",
        "non_inf_mask_test = ~np.isinf(X_test).any(axis=1)\n",
        "mask_test = np.logical_and(non_nan_mask_test,non_inf_mask_test)\n",
        "X_test = X_test[mask_test]\n",
        "\n",
        "# We now reshape the data into shape (num instances, num channels, num timesteps)\n",
        "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
        "print(f\"Test Matrix Shape: {X_test.shape}\")\n",
        "\n",
        "y_test = current_dataset['target_test'].copy()\n",
        "\n",
        "# Remove nans also form labels\n",
        "y_test = y_test[mask_test]\n",
        "\n",
        "# Conver the labels to integers starting from zero\n",
        "y_test = np.asarray([label_to_int[label] for label in y_test])\n",
        "\n",
        "# Print the different unique classes\n",
        "print(f\" \")\n",
        "print(f\"Unique Classes: {np.unique(y_train)}\")\n",
        "number_of_classes = len(np.unique(y_train))\n",
        "\n",
        "# Print the proportion of each class\n",
        "print(f\" \")\n",
        "for i in np.unique(y_train):\n",
        "    print(f\"Class {i} has {np.sum(y_train==i)/len(y_train)*100:.2f}% of the data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBUJP7g_Fdgc"
      },
      "source": [
        "### Plotting the time series data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxgILF-cFdgd"
      },
      "outputs": [],
      "source": [
        "# Plot the 10 instances of each class in a subplot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a dictionary to map the class to a color\n",
        "color_dict = {0:'red',1:'blue',2:'green',3:'yellow',4:'black',5:'orange',6:'purple',7:'pink',8:'brown',9:'cyan', -1:'cyan'}\n",
        "\n",
        "# Plot the first 10 instances\n",
        "plt.figure(figsize=(18,10))\n",
        "for i in range(10):\n",
        "    # pick random instance\n",
        "    idx = np.random.randint(0,X_train.shape[0])\n",
        "    plt.subplot(number_of_classes,5,i+1)\n",
        "    plt.plot(X_train[idx,0],color=color_dict[y_train[idx]])\n",
        "    plt.title(f\"Class: {y_train[idx]}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYtCTmQjFdgd"
      },
      "source": [
        "## 2) Feature Based Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qxcKM-kFdgd"
      },
      "source": [
        "### Create a Function to train and evaluate a Logistic Regression Classifier\n",
        "\n",
        "We will create a simple function to train and evaluate a logistic regression classifier using sci-kit learn library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kufqYXZTFdgd"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Define a function to train and evaluate a logistic regression classifier\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, verbose=True):\n",
        "\n",
        "    # Create the classifier\n",
        "    clf = LogisticRegression(max_iter=1000, penalty='l2',class_weight = 'balanced')\n",
        "    # Yo can try a different classifier (e.g. SVM, random forest, etc.)\n",
        "    # clf = SVC(kernel='rbf',C=1.0)\n",
        "    # clf = RandomForestClassifier(n_estimators=50,max_depth=10)\n",
        "\n",
        "    # Fit the classifier\n",
        "    clf.fit(X_train,y_train)\n",
        "\n",
        "    # Predict the train set\n",
        "    y_pred_train = clf.predict(X_train)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy_train = accuracy_score(y_train,y_pred_train)\n",
        "\n",
        "    # Compute the confusion matrix for the train set\n",
        "    conf_matrix_train = confusion_matrix(y_train,y_pred_train)\n",
        "\n",
        "    # Predict the test set\n",
        "    y_pred_test = clf.predict(X_test)\n",
        "\n",
        "    # Calculate the accuracy\n",
        "    accuracy_test = accuracy_score(y_test,y_pred_test)\n",
        "\n",
        "    # Compute the confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred_test)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Train Accuracy: {accuracy_train}\")\n",
        "        print(f\" \")\n",
        "        print(f\"Confusion Matrix on Train Set: \")\n",
        "        # plot confusion matrix\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix_train,display_labels=np.unique(y_train))\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "        print(f\" \")\n",
        "        print(f\"Test Accuracy: {accuracy_test}\")\n",
        "        print(f\" \")\n",
        "        print(f\"Confusion Matrix on Test Set: \")\n",
        "        # plot confusion matrix\n",
        "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix,display_labels=np.unique(y_test))\n",
        "        disp.plot()\n",
        "        plt.show()\n",
        "\n",
        "    return accuracy_train, accuracy_test, conf_matrix, clf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9hTsZ_aFdge"
      },
      "source": [
        "### A) Classify using TSFresh features\n",
        "We first create the large set of features for each time series using TSFresh."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oS2c1fkQFdge"
      },
      "outputs": [],
      "source": [
        "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
        "\n",
        "# Create TSFresh trasformation\n",
        "ts_fresh_transform = TSFreshFeatureExtractor(default_fc_parameters=\"efficient\", show_warnings=False, disable_progressbar=False)\n",
        "\n",
        "# Fit and transform Time Series\n",
        "X_train_ts = ts_fresh_transform.fit_transform(X_train)\n",
        "X_test_ts = ts_fresh_transform.transform(X_test)\n",
        "\n",
        "X_train_ts = X_train_ts.values\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_train = ~np.isnan(X_train_ts).any(axis=1)\n",
        "non_inf_mask_train = ~np.isinf(X_train_ts).any(axis=1)\n",
        "mask_train = np.logical_and(non_nan_mask_train,non_inf_mask_train)\n",
        "X_train_ts = X_train_ts[mask_train]\n",
        "y_train_ts = current_dataset['target_train'].copy()\n",
        "y_train_ts = y_train_ts[mask_train]\n",
        "\n",
        "X_test_ts = X_test_ts.values\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_test = ~np.isnan(X_test_ts).any(axis=1)\n",
        "non_inf_mask_test = ~np.isinf(X_test_ts).any(axis=1)\n",
        "mask_test = np.logical_and(non_nan_mask_test,non_inf_mask_test)\n",
        "X_test_ts = X_test_ts[mask_test]\n",
        "y_test_ts = current_dataset['target_test'].copy()\n",
        "y_test_ts = y_test_ts[mask_test]\n",
        "\n",
        "print(f\" \")\n",
        "print(f\"TSFresh Features Matrix Shape: ( # of instances , # of features )\")\n",
        "print(f\" \")\n",
        "print(f\"Train: {X_train_ts.shape}\")\n",
        "print(f\" \")\n",
        "print(f\"Test: {X_test_ts.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVtk6VIfFdge"
      },
      "source": [
        "We evaluate how well these features can classify the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqUrSfYuFdge"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the classifier for the features created by TSFresh\n",
        "print(f\"Results for TSFresh:\")\n",
        "print(f\" \")\n",
        "acc_train_ts, acc_test_ts, conf_matrix_ts, classifier_ts = train_and_evaluate(X_train_ts, y_train_ts, X_test_ts, y_test_ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iCGVYwvFdge"
      },
      "source": [
        "### B) Classify using Catch22\n",
        "We now compute the reduced set of features proposed in Catch22."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XgmadFSFdge"
      },
      "outputs": [],
      "source": [
        "from sktime.transformations.panel.catch22 import Catch22\n",
        "\n",
        "# Create Catch22 trasformation\n",
        "catch22_transform = Catch22()\n",
        "\n",
        "# Fit and transform Time Series\n",
        "X_train_catch22 = catch22_transform.fit_transform(X_train)\n",
        "X_test_catch22 = catch22_transform.transform(X_test)\n",
        "\n",
        "X_train_catch22 = X_train_catch22.values\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_train = ~np.isnan(X_train_catch22).any(axis=1)\n",
        "non_inf_mask_train = ~np.isinf(X_train_catch22).any(axis=1)\n",
        "mask_train = np.logical_and(non_nan_mask_train,non_inf_mask_train)\n",
        "X_train_catch22 = X_train_catch22[mask_train]\n",
        "y_train_catch22 = current_dataset['target_train'].copy()\n",
        "y_train_catch22 = y_train_catch22[mask_train]\n",
        "\n",
        "X_test_catch22 = X_test_catch22.values\n",
        "# This part is to make shure there is no nans of inf in the data\n",
        "non_nan_mask_test = ~np.isnan(X_test_catch22).any(axis=1)\n",
        "non_inf_mask_test = ~np.isinf(X_test_catch22).any(axis=1)\n",
        "mask_test = np.logical_and(non_nan_mask_test,non_inf_mask_test)\n",
        "X_test_catch22 = X_test_catch22[mask_test]\n",
        "y_test_catch22 = current_dataset['target_test'].copy()\n",
        "y_test_catch22 = y_test_catch22[mask_test]\n",
        "\n",
        "print(f\" \")\n",
        "print(f\"Catch22 Features Matrix Shape: ( # of instances , # of features )\")\n",
        "print(f\" \")\n",
        "print(f\"Train: {X_train_catch22.shape}\")\n",
        "print(f\" \")\n",
        "print(f\"Test: {X_test_catch22.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HLV7aOlFdge"
      },
      "source": [
        "We evaluate how well these features can classify the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfQGmTsTFdge"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate the classifier for the features created by TSFresh\n",
        "print(f\"RResults for Catach22:\")\n",
        "print(f\" \")\n",
        "acc_train_catch22, acc_test_catch22, conf_matrix_catch22, classifier_catch22 = train_and_evaluate(X_train_catch22, y_train_catch22, X_test_catch22, y_test_catch22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bviOnOOzFdgf"
      },
      "source": [
        "## 3) Deep Learning Method: CNN Model\n",
        "We will train a vanilla CNN architecture to solve the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUMsDHuGFdgf"
      },
      "outputs": [],
      "source": [
        "# The definition of these callbacks is just to store the accuracy values during training process.\n",
        "# There is no need to modify or understand this part of the code.\n",
        "\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "acc_list_train = []\n",
        "acc_list_test = []\n",
        "\n",
        "class AccuracyCallbackTrain(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X, y, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.X = np.swapaxes(X, 1, 2) # Correct the shape if necessary\n",
        "        self.y = y\n",
        "        self.threshold = threshold  # Threshold for binary classification (default 0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Get predictions on the entire training set\n",
        "        predictions = self.model.predict(self.X)\n",
        "        y_pred = predictions.argmax(axis=1)\n",
        "        accuracy = accuracy_score(self.y, y_pred)\n",
        "\n",
        "        # Store accuracy value\n",
        "        acc_list_train.append(accuracy)\n",
        "        #print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "class AccuracyCallbackTest(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, X, y, threshold=0.5):\n",
        "        super().__init__()\n",
        "        self.X = np.swapaxes(X, 1, 2) # Correct the shape if necessary\n",
        "        self.y = y\n",
        "        self.threshold = threshold  # Threshold for binary classification (default 0.5)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "\n",
        "        # Get predictions on the entire training set\n",
        "        predictions = self.model.predict(self.X)\n",
        "        y_pred = predictions.argmax(axis=1)\n",
        "        accuracy = accuracy_score(self.y, y_pred)\n",
        "\n",
        "        # Store accuracy value\n",
        "        acc_list_test.append(accuracy)\n",
        "        #print(f\"Epoch {epoch + 1}: Accuracy = {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TYBx77JFdgf"
      },
      "source": [
        "Let's train the simple CNN model on the raw time series data.\n",
        "\n",
        "By default, this model will posses the following hyperparameters:\n",
        "\n",
        "*   kernel_size = 7\n",
        "*   avg_pool_size = 3\n",
        "*   n_conv_layers = 2\n",
        "*   filter_sizes = [6, 12] (this is the number of kernelsper layer, need to be equal to the number of layers)\n",
        "\n",
        "You can read more in depth about it in the corresponding documentation:\n",
        "https://www.sktime.net/en/latest/api_reference/auto_generated/sktime.classification.deep_learning.CNNClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GYPCEZczFdgf"
      },
      "outputs": [],
      "source": [
        "from sktime.classification.deep_learning import CNNClassifier\n",
        "\n",
        "# Initialize the callbacks (this is to keep track of the learning process while training)\n",
        "train_accuracy_callback = AccuracyCallbackTrain(X_train, y_train)\n",
        "test_accuracy_callback = AccuracyCallbackTest(X_test, y_test)\n",
        "acc_list_train = []\n",
        "acc_list_test = []\n",
        "\n",
        "# Create the classifier\n",
        "cnn_classifier = CNNClassifier(\n",
        "\n",
        "    # PARAMETERS\n",
        "    n_epochs=40, # Number of epochs\n",
        "    #kernel_size = 7,\n",
        "    #avg_pool_size = 3,\n",
        "    #n_conv_layers = 2,\n",
        "    #filter_sizes = [6, 12],\n",
        "\n",
        "    verbose=True, # Print information about the training\n",
        "    callbacks=[train_accuracy_callback,test_accuracy_callback] # Call the custom callbacks we have created\n",
        "    )\n",
        "\n",
        "# Train the model\n",
        "cnn_classifier.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Set Accuracies:\", acc_list_train)\n",
        "print(\"Test Set Accuracies:\", acc_list_test)\n",
        "\n",
        "# Plot both the loss and the accuracies (train and test) in the same graph using two different y-axis.\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "color = 'tab:red'\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss', color=color)\n",
        "ax1.plot(cnn_classifier.history.history['loss'], color=color)\n",
        "ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "color = 'tab:blue'\n",
        "ax2.set_ylabel('Accuracy', color=color)\n",
        "ax2.plot(acc_list_train, color=color, linestyle='dashed', label='Train Accuracy')\n",
        "ax2.plot(acc_list_test, color=color, linestyle='solid', label='Test Accuracy')\n",
        "ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "fig.legend(loc=\"upper right\")\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhwGPoOlFdgf"
      },
      "outputs": [],
      "source": [
        "# Compute and print the accuracy on the train and test set using the final model\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_test_pred = cnn_classifier.predict(X_test)\n",
        "y_train_pred = cnn_classifier.predict(X_train)\n",
        "\n",
        "# Predict the train set\n",
        "y_train_pred = cnn_classifier.predict(X_train)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_train = accuracy_score(y_train,y_train_pred)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy_test = accuracy_score(y_test,y_test_pred)\n",
        "\n",
        "print(f\"Accuracy Train = {accuracy_train:.2f}\")\n",
        "print(f\"Accuracy Test = {accuracy_test:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE6Sk2uyFdgf"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1) Compare both feature based strategies, which one works better for this dataset? Try a different classifier on the Catch22 features. Are you able to reach a better accuracy?\n",
        "\n",
        "3) What happens when you train the CNN model on a large number of epochs? Do more epochs help? Explore the architecture by modifying some of the model's hyperparameters (the default architecture is far from the best!)\n",
        "\n",
        "4) [EXTRA] Implement a recurrent neural network (a GRU classifier). Compare its performance / training time / number of parameters with the CNN. Use the documentation of the SKTime library for help."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "forecast",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}