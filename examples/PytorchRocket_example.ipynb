{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaTES2tMkBAR"
      },
      "source": [
        "## Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMLwcJUJ5ynP",
        "outputId": "f44eda72-1cf6-4ac6-dfa2-59ab001370c6"
      },
      "outputs": [],
      "source": [
        "# install github branch feat/pytorch-detach from  git+https://github.com/gon-uri/detach_rocket\n",
        "\n",
        "!pip install sktime --quiet\n",
        "#!pip install  git+https://github.com/gon-uri/detach_rocket.git@feat/pytorch-detach --quiet\n",
        "!pip install pyts --quiet\n",
        "!pip install matplotlib --quiet\n",
        "!pip install torch --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4WN7u9ql-0h"
      },
      "source": [
        "## Download Dataset from UCR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ruXkm-gumddl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from detach_rocket.utils_datasets import fetch_ucr_dataset\n",
        "\n",
        "# Download Dataset\n",
        "dataset_name_list = ['ProximalPhalanxOutlineAgeGroup'] # PhalangesOutlinesCorrect ProximalPhalanxOutlineCorrect #Fordb\n",
        "current_dataset = fetch_ucr_dataset(dataset_name_list[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgzA1-1tci-q"
      },
      "source": [
        "## Prepare Dataset Matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmzM3E_OxaU1",
        "outputId": "f1179dc7-5864-4db1-d9c1-0519f323dd7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Matrix Shape: ( # of instances , time series length )\n",
            " \n",
            "Train: (400, 1, 80)\n",
            " \n",
            "Test: (205, 1, 80)\n",
            " \n",
            "Number of classes: 3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Create data matrices and remove possible rows with nans\n",
        "\n",
        "print(f\"Dataset Matrix Shape: ( # of instances , time series length )\")\n",
        "print(f\" \")\n",
        "\n",
        "# Train Matrix\n",
        "X_train = current_dataset['data_train']\n",
        "\n",
        "non_nan_mask_train = ~np.isnan(X_train).any(axis=1)\n",
        "non_inf_mask_train = ~np.isinf(X_train).any(axis=1)\n",
        "mask_train = np.logical_and(non_nan_mask_train,non_inf_mask_train)\n",
        "X_train = X_train[mask_train]\n",
        "X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
        "y_train = current_dataset['target_train']\n",
        "y_train = y_train[mask_train]\n",
        "print(f\"Train: {X_train.shape}\")\n",
        "\n",
        "print(f\" \")\n",
        "\n",
        "# Test Matrix\n",
        "X_test = current_dataset['data_test']\n",
        "#print(f\"Number of test instances: {len(X_test)}\")\n",
        "\n",
        "non_nan_mask_test = ~np.isnan(X_test).any(axis=1)\n",
        "non_inf_mask_test = ~np.isinf(X_test).any(axis=1)\n",
        "mask_test = np.logical_and(non_nan_mask_test,non_inf_mask_test)\n",
        "X_test = X_test[mask_test]\n",
        "X_test = X_test.reshape(X_test.shape[0],1,X_test.shape[1])\n",
        "y_test = current_dataset['target_test']\n",
        "y_test = y_test[mask_test]\n",
        "print(f\"Test: {X_test.shape}\")\n",
        "\n",
        "print(f\" \")\n",
        "# Number of classes\n",
        "n_classes = len(np.unique(y_train))\n",
        "print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "# Convert y_train and y_test to start from 0\n",
        "y_train = y_train - np.min(y_train)\n",
        "y_test = y_test - np.min(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkgnHeNtmFec"
      },
      "source": [
        "## Train and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5uOBpflHKjJ",
        "outputId": "f28e5bb3-ed41-4fc8-e42f-4496de73274f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7, 9, 11]\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "from detach_rocket.detach_classes import RocketFeaturesPytorch, RocketPytorch\n",
        "\n",
        "\n",
        "# Import torch and early stopping\n",
        "import torch\n",
        "\n",
        "np.random.seed(2)\n",
        "\n",
        "# Select initial model characteristics\n",
        "c_in = X_train.shape[1]\n",
        "\n",
        "# If binary classification, c_out = 1, else c_out = n_classes\n",
        "if n_classes == 2:\n",
        "    c_out = 1\n",
        "else:\n",
        "    c_out = n_classes\n",
        "\n",
        "seq_len = X_train.shape[2]\n",
        "num_kernels=1000\n",
        "L2_regularization_weight = 1e-3\n",
        "\n",
        "# Create model object\n",
        "PytorchRocketModel = RocketPytorch(c_in,c_out,seq_len,num_kernels)\n",
        "PytorchRocketModel = PytorchRocketModel.float()\n",
        "\n",
        "# Define early stopping class\n",
        "class EarlyStopping:\n",
        "    \"\"\"\n",
        "    Early stops the training if validation loss doesn't improve after a given patience.\n",
        "    \"\"\"\n",
        "    def __init__(self, patience=20, verbose=False, delta=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 20\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score - self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else: \n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "# Create DataLoaders\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 2048\n",
        "\n",
        "# Define train function for Pytorch model with epochs and early stopping\n",
        "def train_model(model, train_data, test_data, batch_size, patience):\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    # Create DataLoaders\n",
        "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=False)\n",
        "    test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=False)\n",
        "    # Define loss function and optimizer (for multi-class classification)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=L2_regularization_weight)\n",
        "    # Define early stopping\n",
        "    #early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
        "    # Define number of epochs\n",
        "    n_epochs = 100\n",
        "    # Initialize lists for train and test losses\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    # Train model\n",
        "    for epoch in range(n_epochs):\n",
        "        # Initialize variables to monitor training and test loss\n",
        "        train_loss = 0.0\n",
        "        test_loss = 0.0\n",
        "        # Train model\n",
        "        model.train()\n",
        "        for data, target in train_loader:\n",
        "            # Clear the gradients of all optimized variables\n",
        "            optimizer.zero_grad()\n",
        "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data.float())\n",
        "            # Calculate the batch loss\n",
        "            target = target.long()\n",
        "            # If not multi-class classification, target needs to be reshaped\n",
        "            if c_out == 1:\n",
        "                target = target.unsqueeze(1)\n",
        "            #print('output: ',output.shape)\n",
        "            #print('target: ',target.shape)\n",
        "            loss = criterion(output, target)\n",
        "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
        "            loss.backward()\n",
        "            # Perform a single optimization step (parameter update)\n",
        "            optimizer.step()\n",
        "            # Update training loss\n",
        "            train_loss += loss.item()*data.size(0)\n",
        "        # Validate model\n",
        "        model.eval()\n",
        "        for data, target in test_loader:\n",
        "            # Forward pass: compute predicted outputs by passing inputs to the model\n",
        "            output = model(data.float())\n",
        "            # Calculate the batch loss\n",
        "            target = target.long()\n",
        "            if c_out == 1:\n",
        "                target = target.unsqueeze(1)\n",
        "            loss = criterion(output, target)\n",
        "            # Update average test loss \n",
        "            test_loss += loss.item()*data.size(0)\n",
        "        # Calculate average losses\n",
        "        train_loss = train_loss/len(train_loader.dataset)\n",
        "        test_loss = test_loss/len(test_loader.dataset)\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "        # Print training and test results\n",
        "        print(f'Epoch: {epoch+1} \\tTraining Loss: {train_loss:.6f} \\tValidation Loss: {test_loss:.6f}')\n",
        "        # Early stop\n",
        "        early_stopping(test_loss, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 \tTraining Loss: 1.098612 \tValidation Loss: 2.045879\n",
            "Validation loss decreased (inf --> 2.045879).  Saving model ...\n",
            "Epoch: 2 \tTraining Loss: 0.619119 \tValidation Loss: 2.745318\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 3 \tTraining Loss: 0.621798 \tValidation Loss: 3.048682\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 4 \tTraining Loss: 0.653161 \tValidation Loss: 2.957419\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 5 \tTraining Loss: 0.649564 \tValidation Loss: 2.614972\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 6 \tTraining Loss: 0.612249 \tValidation Loss: 2.140547\n",
            "EarlyStopping counter: 5 out of 20\n",
            "Epoch: 7 \tTraining Loss: 0.551696 \tValidation Loss: 1.628092\n",
            "Validation loss decreased (2.045879 --> 1.628092).  Saving model ...\n",
            "Epoch: 8 \tTraining Loss: 0.479001 \tValidation Loss: 1.163137\n",
            "Validation loss decreased (1.628092 --> 1.163137).  Saving model ...\n",
            "Epoch: 9 \tTraining Loss: 0.412116 \tValidation Loss: 0.829146\n",
            "Validation loss decreased (1.163137 --> 0.829146).  Saving model ...\n",
            "Epoch: 10 \tTraining Loss: 0.377527 \tValidation Loss: 0.671210\n",
            "Validation loss decreased (0.829146 --> 0.671210).  Saving model ...\n",
            "Epoch: 11 \tTraining Loss: 0.389172 \tValidation Loss: 0.646373\n",
            "Validation loss decreased (0.671210 --> 0.646373).  Saving model ...\n",
            "Epoch: 12 \tTraining Loss: 0.421455 \tValidation Loss: 0.656488\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 13 \tTraining Loss: 0.434801 \tValidation Loss: 0.642893\n",
            "Validation loss decreased (0.646373 --> 0.642893).  Saving model ...\n",
            "Epoch: 14 \tTraining Loss: 0.415635 \tValidation Loss: 0.604409\n",
            "Validation loss decreased (0.642893 --> 0.604409).  Saving model ...\n",
            "Epoch: 15 \tTraining Loss: 0.379127 \tValidation Loss: 0.560410\n",
            "Validation loss decreased (0.604409 --> 0.560410).  Saving model ...\n",
            "Epoch: 16 \tTraining Loss: 0.346968 \tValidation Loss: 0.524225\n",
            "Validation loss decreased (0.560410 --> 0.524225).  Saving model ...\n",
            "Epoch: 17 \tTraining Loss: 0.330802 \tValidation Loss: 0.498436\n",
            "Validation loss decreased (0.524225 --> 0.498436).  Saving model ...\n",
            "Epoch: 18 \tTraining Loss: 0.329915 \tValidation Loss: 0.480700\n",
            "Validation loss decreased (0.498436 --> 0.480700).  Saving model ...\n",
            "Epoch: 19 \tTraining Loss: 0.336403 \tValidation Loss: 0.468939\n",
            "Validation loss decreased (0.480700 --> 0.468939).  Saving model ...\n",
            "Epoch: 20 \tTraining Loss: 0.341855 \tValidation Loss: 0.462703\n",
            "Validation loss decreased (0.468939 --> 0.462703).  Saving model ...\n",
            "Epoch: 21 \tTraining Loss: 0.341287 \tValidation Loss: 0.462496\n",
            "Validation loss decreased (0.462703 --> 0.462496).  Saving model ...\n",
            "Epoch: 22 \tTraining Loss: 0.333730 \tValidation Loss: 0.468852\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 23 \tTraining Loss: 0.321211 \tValidation Loss: 0.481666\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 24 \tTraining Loss: 0.307439 \tValidation Loss: 0.499656\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 25 \tTraining Loss: 0.296377 \tValidation Loss: 0.519824\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 26 \tTraining Loss: 0.290544 \tValidation Loss: 0.537460\n",
            "EarlyStopping counter: 5 out of 20\n",
            "Epoch: 27 \tTraining Loss: 0.289644 \tValidation Loss: 0.547418\n",
            "EarlyStopping counter: 6 out of 20\n",
            "Epoch: 28 \tTraining Loss: 0.290702 \tValidation Loss: 0.546264\n",
            "EarlyStopping counter: 7 out of 20\n",
            "Epoch: 29 \tTraining Loss: 0.289782 \tValidation Loss: 0.533953\n",
            "EarlyStopping counter: 8 out of 20\n",
            "Epoch: 30 \tTraining Loss: 0.284413 \tValidation Loss: 0.513978\n",
            "EarlyStopping counter: 9 out of 20\n",
            "Epoch: 31 \tTraining Loss: 0.275403 \tValidation Loss: 0.491776\n",
            "EarlyStopping counter: 10 out of 20\n",
            "Epoch: 32 \tTraining Loss: 0.266199 \tValidation Loss: 0.472240\n",
            "EarlyStopping counter: 11 out of 20\n",
            "Epoch: 33 \tTraining Loss: 0.259926 \tValidation Loss: 0.457994\n",
            "Validation loss decreased (0.462496 --> 0.457994).  Saving model ...\n",
            "Epoch: 34 \tTraining Loss: 0.257105 \tValidation Loss: 0.449340\n",
            "Validation loss decreased (0.457994 --> 0.449340).  Saving model ...\n",
            "Epoch: 35 \tTraining Loss: 0.255959 \tValidation Loss: 0.445265\n",
            "Validation loss decreased (0.449340 --> 0.445265).  Saving model ...\n",
            "Epoch: 36 \tTraining Loss: 0.254156 \tValidation Loss: 0.444472\n",
            "Validation loss decreased (0.445265 --> 0.444472).  Saving model ...\n",
            "Epoch: 37 \tTraining Loss: 0.250299 \tValidation Loss: 0.445952\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 38 \tTraining Loss: 0.244506 \tValidation Loss: 0.449057\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 39 \tTraining Loss: 0.238140 \tValidation Loss: 0.453196\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 40 \tTraining Loss: 0.232904 \tValidation Loss: 0.457396\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 41 \tTraining Loss: 0.229697 \tValidation Loss: 0.460150\n",
            "EarlyStopping counter: 5 out of 20\n",
            "Epoch: 42 \tTraining Loss: 0.227950 \tValidation Loss: 0.459928\n",
            "EarlyStopping counter: 6 out of 20\n",
            "Epoch: 43 \tTraining Loss: 0.226086 \tValidation Loss: 0.456152\n",
            "EarlyStopping counter: 7 out of 20\n",
            "Epoch: 44 \tTraining Loss: 0.222892 \tValidation Loss: 0.449757\n",
            "EarlyStopping counter: 8 out of 20\n",
            "Epoch: 45 \tTraining Loss: 0.218539 \tValidation Loss: 0.442674\n",
            "Validation loss decreased (0.444472 --> 0.442674).  Saving model ...\n",
            "Epoch: 46 \tTraining Loss: 0.214216 \tValidation Loss: 0.436655\n",
            "Validation loss decreased (0.442674 --> 0.436655).  Saving model ...\n",
            "Epoch: 47 \tTraining Loss: 0.210919 \tValidation Loss: 0.432475\n",
            "Validation loss decreased (0.436655 --> 0.432475).  Saving model ...\n",
            "Epoch: 48 \tTraining Loss: 0.208697 \tValidation Loss: 0.429938\n",
            "Validation loss decreased (0.432475 --> 0.429938).  Saving model ...\n",
            "Epoch: 49 \tTraining Loss: 0.206853 \tValidation Loss: 0.428390\n",
            "Validation loss decreased (0.429938 --> 0.428390).  Saving model ...\n",
            "Epoch: 50 \tTraining Loss: 0.204613 \tValidation Loss: 0.427234\n",
            "Validation loss decreased (0.428390 --> 0.427234).  Saving model ...\n",
            "Epoch: 51 \tTraining Loss: 0.201680 \tValidation Loss: 0.426179\n",
            "Validation loss decreased (0.427234 --> 0.426179).  Saving model ...\n",
            "Epoch: 52 \tTraining Loss: 0.198338 \tValidation Loss: 0.425182\n",
            "Validation loss decreased (0.426179 --> 0.425182).  Saving model ...\n",
            "Epoch: 53 \tTraining Loss: 0.195152 \tValidation Loss: 0.424222\n",
            "Validation loss decreased (0.425182 --> 0.424222).  Saving model ...\n",
            "Epoch: 54 \tTraining Loss: 0.192509 \tValidation Loss: 0.423143\n",
            "Validation loss decreased (0.424222 --> 0.423143).  Saving model ...\n",
            "Epoch: 55 \tTraining Loss: 0.190345 \tValidation Loss: 0.421709\n",
            "Validation loss decreased (0.423143 --> 0.421709).  Saving model ...\n",
            "Epoch: 56 \tTraining Loss: 0.188247 \tValidation Loss: 0.419840\n",
            "Validation loss decreased (0.421709 --> 0.419840).  Saving model ...\n",
            "Epoch: 57 \tTraining Loss: 0.185845 \tValidation Loss: 0.417777\n",
            "Validation loss decreased (0.419840 --> 0.417777).  Saving model ...\n",
            "Epoch: 58 \tTraining Loss: 0.183121 \tValidation Loss: 0.415968\n",
            "Validation loss decreased (0.417777 --> 0.415968).  Saving model ...\n",
            "Epoch: 59 \tTraining Loss: 0.180364 \tValidation Loss: 0.414774\n",
            "Validation loss decreased (0.415968 --> 0.414774).  Saving model ...\n",
            "Epoch: 60 \tTraining Loss: 0.177867 \tValidation Loss: 0.414252\n",
            "Validation loss decreased (0.414774 --> 0.414252).  Saving model ...\n",
            "Epoch: 61 \tTraining Loss: 0.175689 \tValidation Loss: 0.414174\n",
            "Validation loss decreased (0.414252 --> 0.414174).  Saving model ...\n",
            "Epoch: 62 \tTraining Loss: 0.173667 \tValidation Loss: 0.414200\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 63 \tTraining Loss: 0.171596 \tValidation Loss: 0.414065\n",
            "Validation loss decreased (0.414174 --> 0.414065).  Saving model ...\n",
            "Epoch: 64 \tTraining Loss: 0.169393 \tValidation Loss: 0.413675\n",
            "Validation loss decreased (0.414065 --> 0.413675).  Saving model ...\n",
            "Epoch: 65 \tTraining Loss: 0.167127 \tValidation Loss: 0.413084\n",
            "Validation loss decreased (0.413675 --> 0.413084).  Saving model ...\n",
            "Epoch: 66 \tTraining Loss: 0.164932 \tValidation Loss: 0.412425\n",
            "Validation loss decreased (0.413084 --> 0.412425).  Saving model ...\n",
            "Epoch: 67 \tTraining Loss: 0.162894 \tValidation Loss: 0.411838\n",
            "Validation loss decreased (0.412425 --> 0.411838).  Saving model ...\n",
            "Epoch: 68 \tTraining Loss: 0.160994 \tValidation Loss: 0.411446\n",
            "Validation loss decreased (0.411838 --> 0.411446).  Saving model ...\n",
            "Epoch: 69 \tTraining Loss: 0.159148 \tValidation Loss: 0.411359\n",
            "Validation loss decreased (0.411446 --> 0.411359).  Saving model ...\n",
            "Epoch: 70 \tTraining Loss: 0.157282 \tValidation Loss: 0.411668\n",
            "EarlyStopping counter: 1 out of 20\n",
            "Epoch: 71 \tTraining Loss: 0.155386 \tValidation Loss: 0.412416\n",
            "EarlyStopping counter: 2 out of 20\n",
            "Epoch: 72 \tTraining Loss: 0.153501 \tValidation Loss: 0.413562\n",
            "EarlyStopping counter: 3 out of 20\n",
            "Epoch: 73 \tTraining Loss: 0.151677 \tValidation Loss: 0.414982\n",
            "EarlyStopping counter: 4 out of 20\n",
            "Epoch: 74 \tTraining Loss: 0.149936 \tValidation Loss: 0.416504\n",
            "EarlyStopping counter: 5 out of 20\n",
            "Epoch: 75 \tTraining Loss: 0.148268 \tValidation Loss: 0.417952\n",
            "EarlyStopping counter: 6 out of 20\n",
            "Epoch: 76 \tTraining Loss: 0.146644 \tValidation Loss: 0.419202\n",
            "EarlyStopping counter: 7 out of 20\n",
            "Epoch: 77 \tTraining Loss: 0.145035 \tValidation Loss: 0.420214\n",
            "EarlyStopping counter: 8 out of 20\n",
            "Epoch: 78 \tTraining Loss: 0.143433 \tValidation Loss: 0.421031\n",
            "EarlyStopping counter: 9 out of 20\n",
            "Epoch: 79 \tTraining Loss: 0.141848 \tValidation Loss: 0.421757\n",
            "EarlyStopping counter: 10 out of 20\n",
            "Epoch: 80 \tTraining Loss: 0.140300 \tValidation Loss: 0.422517\n",
            "EarlyStopping counter: 11 out of 20\n",
            "Epoch: 81 \tTraining Loss: 0.138804 \tValidation Loss: 0.423410\n",
            "EarlyStopping counter: 12 out of 20\n",
            "Epoch: 82 \tTraining Loss: 0.137359 \tValidation Loss: 0.424497\n",
            "EarlyStopping counter: 13 out of 20\n",
            "Epoch: 83 \tTraining Loss: 0.135950 \tValidation Loss: 0.425792\n",
            "EarlyStopping counter: 14 out of 20\n",
            "Epoch: 84 \tTraining Loss: 0.134559 \tValidation Loss: 0.427275\n",
            "EarlyStopping counter: 15 out of 20\n",
            "Epoch: 85 \tTraining Loss: 0.133180 \tValidation Loss: 0.428904\n",
            "EarlyStopping counter: 16 out of 20\n",
            "Epoch: 86 \tTraining Loss: 0.131819 \tValidation Loss: 0.430622\n",
            "EarlyStopping counter: 17 out of 20\n",
            "Epoch: 87 \tTraining Loss: 0.130487 \tValidation Loss: 0.432359\n",
            "EarlyStopping counter: 18 out of 20\n",
            "Epoch: 88 \tTraining Loss: 0.129194 \tValidation Loss: 0.434047\n",
            "EarlyStopping counter: 19 out of 20\n",
            "Epoch: 89 \tTraining Loss: 0.127935 \tValidation Loss: 0.435626\n",
            "EarlyStopping counter: 20 out of 20\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "# Train model\n",
        "train_model(PytorchRocketModel, train_data, test_data, batch_size, patience=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train accuracy: 0.9275\n",
            "Test accuracy: 0.8536585365853658\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "PytorchRocketModel.load_state_dict(torch.load('checkpoint.pt'))\n",
        "\n",
        "# Compute model accuracy both on train and test set\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_data, shuffle=False, batch_size=batch_size, drop_last=False)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=False)\n",
        "\n",
        "# Initialize lists for train and test predictions\n",
        "train_predictions = []\n",
        "test_predictions = []\n",
        "\n",
        "# Compute train predictions\n",
        "PytorchRocketModel.eval()\n",
        "# If it is a binary classification problem, convert probabilities to class labels\n",
        "if len(np.unique(y_train)) == 2:\n",
        "    for data, target in train_loader:\n",
        "        output = PytorchRocketModel(data.float())\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.detach().numpy()\n",
        "        # Convert probabilities to class labels\n",
        "        output = np.where(output > 0.5, 1, 0)\n",
        "        train_predictions.extend(output)\n",
        "# If it is a multiclass classification problem, select class with maximum probability\n",
        "else:\n",
        "    for data, target in train_loader:\n",
        "        output = PytorchRocketModel(data.float())\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.detach().numpy()\n",
        "        # Select class with maximum probability\n",
        "        output = np.argmax(output, axis=1)\n",
        "        # Convert probabilities to class labels\n",
        "        train_predictions.extend(output)\n",
        "\n",
        "# Compute test predictions \n",
        "PytorchRocketModel.eval()\n",
        "# If it is a binary classification problem, convert probabilities to class labels\n",
        "if len(np.unique(y_train)) == 2:\n",
        "    for data, target in test_loader:\n",
        "        output = PytorchRocketModel(data.float())\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.detach().numpy()\n",
        "        # Convert probabilities to class labels\n",
        "        output = np.where(output > 0.5, 1, 0)\n",
        "        test_predictions.extend(output)\n",
        "# If it is a multiclass classification problem, select class with maximum probability\n",
        "else:\n",
        "    for data, target in test_loader:\n",
        "        output = PytorchRocketModel(data.float())\n",
        "        output = torch.sigmoid(output)\n",
        "        output = output.detach().numpy()\n",
        "        # Select class with maximum probability\n",
        "        output = np.argmax(output, axis=1)\n",
        "        # Convert probabilities to class labels\n",
        "        test_predictions.extend(output)\n",
        "\n",
        "# Compute accuracy\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "print(f\"Train accuracy: {train_accuracy}\")\n",
        "print(f\"Test accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_feature_importance(model, multilabel_type=\"max\"):\n",
        "    \"\"\"\n",
        "    Compute feature importance of a trained model.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model.\n",
        "    - multilabel_type: Type of feature ranking in case of multilabel classification (\"max\" by default).\n",
        "\n",
        "    Returns:\n",
        "    - feature_importance: Feature importance vector.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Get last layer weights\n",
        "    last_layer_weights = model.head[2].weight.detach().numpy()\n",
        "\n",
        "    # Check if it is a binary or multiclass classification problem looking at number of dimensions of last layer weights\n",
        "    if len(last_layer_weights.shape) > 1:\n",
        "        if multilabel_type == \"norm\":\n",
        "            feature_importance   = np.linalg.norm(last_layer_weights,axis=0,ord=2)\n",
        "        elif multilabel_type == \"max\":\n",
        "            feature_importance  = np.linalg.norm(last_layer_weights,axis=0,ord=np.inf)\n",
        "        elif multilabel_type == \"avg\":\n",
        "            feature_importance= np.linalg.norm(last_layer_weights,axis=0,ord=1)\n",
        "        else:\n",
        "            raise ValueError('Invalid multilabel_type argument. Choose from: \"norm\", \"max\", or \"avg\".')\n",
        "    else:\n",
        "        feature_importance = np.abs(last_layer_weights)\n",
        "    return feature_importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute feature importance\n",
        "feature_importance = get_feature_importance(PytorchRocketModel, multilabel_type=\"max\")\n",
        "\n",
        "# Compute number of features\n",
        "n_features = len(feature_importance)\n",
        "\n",
        "# Percentage of features to drop\n",
        "drop_percentage = 0.05\n",
        "\n",
        "# Compute number of features to drop\n",
        "n_features_to_drop = int(np.floor(drop_percentage*n_features))\n",
        "\n",
        "# Create mask of features to drop\n",
        "feature_mask = np.zeros(len(feature_importance), dtype=bool)\n",
        "feature_mask[np.argsort(feature_importance)[:n_features_to_drop]] = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of zeros in weight matrix: 0.05 %\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'bool' object does not support item assignment",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/uribarri/Documents/detach_rocket/examples/PytorchRocket_example.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/uribarri/Documents/detach_rocket/examples/PytorchRocket_example.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m weights_matrix \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mParameter(weights_matrix\u001b[39m.\u001b[39mfloat())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/uribarri/Documents/detach_rocket/examples/PytorchRocket_example.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m PytorchRocketModel\u001b[39m.\u001b[39mhead[\u001b[39m2\u001b[39m]\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m weights_matrix\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/uribarri/Documents/detach_rocket/examples/PytorchRocket_example.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m PytorchRocketModel\u001b[39m.\u001b[39;49mhead[\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mrequires_grad[\u001b[39m~\u001b[39;49mweight_mask] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'bool' object does not support item assignment"
          ]
        }
      ],
      "source": [
        "# Create a weigth mask\n",
        "weight_mask = np.zeros((c_out,len(feature_importance)), dtype=bool)\n",
        "for i in range(c_out):\n",
        "    weight_mask[i,:] = feature_mask\n",
        "\n",
        "# Create weight matrix sending to zero weights of dropped features\n",
        "weights_matrix = np.ones((c_out,len(feature_importance)))\n",
        "weights_matrix[weight_mask] = 0\n",
        "weights_matrix = np.multiply(PytorchRocketModel.head[2].weight.detach().numpy(),weights_matrix)\n",
        "\n",
        "# Print amount of zeros in weight matrix\n",
        "print(f\"Percentage of zeros in weight matrix: {np.sum(weights_matrix == 0)/(c_out*n_features)} %\")\n",
        "\n",
        "# Convert weights_matrix to torch torch.nn.parameter \n",
        "weights_matrix = torch.from_numpy(weights_matrix)\n",
        "weights_matrix = torch.nn.Parameter(weights_matrix.float())\n",
        "\n",
        "PytorchRocketModel.head[2].weight = weights_matrix\n",
        "\n",
        "# ACA ESTA EL ERROR\n",
        "# Esto no anda, no se puede poner requires_grad para cada peso\n",
        "PytorchRocketModel.head[2].weight.requires_grad[~weight_mask] = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 226,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PytorchRocketModel.head[2].weight.requires_grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2000])"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PytorchRocketModel.head[2].weight.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 2000)"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Esto lo habia empezado a escribir para cuando haya que selencionar kernels a partir de features\n",
        "\n",
        "# Create a kernel mask\n",
        "kernel_mask_list = []\n",
        "for i in range(int(n_features/2)):\n",
        "    # If two consecutive features are dropped, drop the corresponding kernel\n",
        "    if feature_mask[2*i] == False and feature_mask[2*i+1] == False:\n",
        "        kernel_mask_list.append(False)\n",
        "    else:\n",
        "        kernel_mask_list.append(True)\n",
        "\n",
        "# Drop kernels\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
